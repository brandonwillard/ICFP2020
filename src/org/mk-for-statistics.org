#+TITLE: miniKanren and Statistical Computation
#+AUTHOR: Brandon T. Willard
#+DATE: 2020-05-09
#+EMAIL: brandonwillard@gmail.com
#+FILETAGS: :minikanren:pymc:symbolic-pymc:statistics:relational-programming:

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: author:t date:t ^:nil toc:nil title:t tex:t d:(not "todo" "logbook" "note" "testing" "notes") html-preamble:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+BEGIN_SRC elisp :eval yes :exports none :results silent
(add-to-list 'org-latex-classes
             '("amsart" "\\documentclass[11pt]{amsart}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+END_SRC

#+SETUPFILE: latex-setup.org

#+PROPERTY: header-args :eval never-export :exports both :results output drawer replace
#+PROPERTY: header-args+ :session dlm-optimizations :comments noweb
#+PROPERTY: header-args:python :noweb-sep "\n\n"
#+PROPERTY: header-args:latex :results raw replace :exports results :eval yes

#+BEGIN_abstract
This article provides a brief overview of the current use and future potential
of symbolic computation within the statistical modeling and machine learning
community of Python.  It details the use of miniKanren as an underlying
framework for term rewriting on top of existing Python libraries and its unique
potential for exciting cross-overs between the currently disparate areas of
statistical modeling and programming language theory.
#+END_abstract

* Introduction
- Probabilistic Programming Languages (PPLs) are DSLs that model elements of linear algebra and probability theory
- PPLs are often backed by "Tensor libraries"
- Tensor libraries are the somewhat more modern equivalents of LAPACK and BLAS-type offerings.
- They use symbolic graphs
* Symbolic Computation in Modern Statistics and Machine Learning
:PROPERTIES:
:CUSTOM_ID: sec:intro-symbolic-computation
:END:
There is an appreciable amount of symbolic computation behind the standard Deep
Learning libraries of today, like Theano [[citep:bergstra_theano:_2010]],
TensorFlow [[citep:TensorFlow2020]], and PyTorch [[citep:PyTorch2020]].

At the very least, these libraries provide classes that represent tensor algebra
in graphical form and functions that manipulate graphs.  Furthermore, these
graphs are used to compute derivatives--via Automatic Differentiation (AD),
parallelize or vectorize operations, and, in limited cases, explicitly perform
algebraic simplifications.

Regardless of whether or not you're in a camp that says AD falls well within
standard symbolic mathematics, these libraries are undoubtedly performing
symbolic calculations, and most of them are on a path toward even more symbolic
computation and outright symbolic mathematics.

Theano's optimizations are perhaps the best example of this.  It provides an
entire subsystem for incorporating "graph optimizations" that effectively employ
common algebraic simplifications.  Ultimately, this optimization systems is a
mix of standard term rewriting and symbolic algebra (e.g. pattern matching, term
substitutions, common subset elimination, graph traversal strategies,
fixed-point operators, etc.)

Although Theano is the older and more established tensor library and has easily
demonstrated the efficacy of its optimizations, TensorFlow and others do not
appear to have nearly as much design and development focus in the areas of
symbolic mathematics and term rewriting.  Even so, there are projects and
communities within each that acknowledge the importance of broader symbolic
computation and are gradually developing toward Theano's earlier example
[[citep:LarsenTensorFlowGraphOptimizations2019a]].

Outside of tensor libraries, projects like JAX [[citep:JAX2020]] approach similar
symbolic-oriented operations by way of traced Python code and Python AST.

Overall, at this point in time, there is a fairly well established set of modern
machine learning and statistical modeling libraries that are all fundamentally
built upon the basics of symbolic manipulation of graphs representing
tensor/linear algebra operations.  They all vary in the degree to which they
implement symbolic mathematics or programmatically encode the underlying
high-level mathematics.  Regardless, the case for more advanced symbolic
computation and mathematics is slowing being made by multiple influential
projects, and there's already enough reason and prior work to start assessing
the directions this could go, and how we might get the most out of it.

Ideally, symbolic work within the machine learning and statistical modeling
community would progress by developing constructively on top of well established
projects, so that quality code can be reused, existing expertise can be
leveraged, and community involvement from domain experts is easily incorporated.

Within the Python community, [[citet:RocklinMathematicallyinformedlinear2013]]
states this sentiment with regards to the intelligent use of optimized linear
algebra functions and the basics of term rewriting.  This work follows the same
principles, but focuses on the areas of statistical modeling and, more
specifically, Bayesian modeling.  Still, in order to reach the area of
statistical modeling, the same foundational properties are required: flexible,
compartmentalized, and open systems that are easily integrated with established
libraries and map well to their high-level abstractions.

Most symbolic mathematics libraries do not have these properties.  Instead, they
require one to work entirely within the limited ecosystems they provide, and, as
a result restrict their application within more advanced--albeit narrower--libraries
that specialize in, say, tensor algebra.  Often, only a small portion of a
symbolic mathematics library's functionality is relevant, but it isn't possible
to use these portions independently.  Worse yet, some libraries are literally
their own programming languages and present an unjustifiable barrier to their
integration within other systems.

* What we want to do in Statistical Modeling
:PROPERTIES:
:CUSTOM_ID: sec:want-to-do
:END:

We simply want a framework within which we can automate "optimizations" specific
to the domains of statistical modeling and machine learning.  We would like this
framework to have the properties outlined in Section
[[#sec:intro-symbolic-computation]] and build upon the well established Python
machine learning and statistics ecosystem.

These automations should be usable internally by new and existing projects to
drive more advanced automations, as well as *interactively* by researchers
interested in developing new model estimation techniques.  In the latter
capacity, the framework we describe is more akin to the standard symbolic
mathematics offerings; however, this consideration should only emphasize the
degree to which the framework we desire is expected to integrate with existing
libraries, since we expect this integration to apply equally to existing
symbolic mathematics libraries.

Now, let's consider statistical modeling explicitly.

Statistical modeling is surprisingly amenable to symbolic methods
[[citep:CaretteModelmanipulationpart2007,
CaretteSimplifyingProbabilisticPrograms2016]]--and especially when one is
working within the more restricted context of Bayesian models
[[citep:ShanExactBayesianInference2017]], where a fundamental principle called
"conjugacy" [[citep:robert_bayesian_2007][Chapter 3.3]] is often used and entirely amenable
to simple pattern matching.  Just within the realm of sample-based model
estimation, there are numerous principles and generalized results that are
applicable to large classes of models.

Regarding principles, one of the most basic is Rao-Blackwellization.
Rao-Blackwellization is derived from the Rao-Blackwell Theorem
[[citep:CasellaRaoBlackwellisationsamplingschemes1996]], which states that
analytically computed conditional expectations outperform their un-integrated
counterparts.  It applies in a rather general sense to numerous Markov Chain
Monte Carlo methods, but it's automation is something that falls well outside of
most applied libraries due to its symbolic computation requirements.

Those requirements aren't necessarily on the means or requirements of symbolic
computation as much as they are on the breadth of coverage.  For instance, if
one wants to apply as much Rao-Blackwellization as possible, they would have to
employ a state-of-the-art symbolic integration routine over arbitrary integrals
implied by the applicable statistical models.  At the level of (nearly) arbitrary
integrals, this problem is notoriously difficult.

Instead, it would be much more convenient and approachable to establish
higher-level relationships that are well known to practitioners and cover
specific classes of models or distributions, then combine those in a way that
boosts their coverage over time.  This is something between the extreme
of enumerate explicit simplifications for each and every possible model and
stating extremely general axioms from which these simplifications can be
derived.  We like to constructively navigate those two extremes.

For instance, Rao-Blackwellizations appearing in published material is often
driven by high-level *relations*.  Some of those relations involve basic theorems
in probability theory like the following algebraic fact about the sum of
Gaussian random variables:
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:sum-of-normals}
  \begin{gathered}
    X + Y = Z
    \\
    X \sim \operatorname{N}\mleft(\mu_x, \sigma_x^2\mright), \quad
    Y \sim \operatorname{N}\mleft(\mu_y, \sigma_y^2\mright), \quad
    Z \sim \operatorname{N}\mleft(\mu_x + \mu_y, \sigma_x^2 + \sigma_y^2\mright)
  \end{gathered}
\end{equation}
#+END_SRC

There are numerous examples like [[eqref:eq:sum-of-normals]], and they all take the
form of relations.  Simple algebraic relations like these can be combined to
produce derivations that effectively state the closed-form integrals underlying
Rao-Blackwellization.

This general idea mirrors the approaches used by modern symbolic integration
systems themselves.  When such systems employ Fox H and Meijer G functions
[[citep:peasgood_method_2009, roach_meijer_1997]], they are effectively
using only a few simple algebraic convolution relations applying to broad
classes of hypergeometric functions.  While those systems are intended to reach
a much greater number of functions, a majority of the work being done by
statistical modeling systems deals with a much smaller set of standard
distributions.

Currently, statistical modeling systems do not cater to these types of
additions, nor do they attempt to employ general optimizations like
Rao-Blackwellization.  Instead, this kind of work is still restricted to the
user-level, where it is performed by hand and used as input to the system.  At
the present, the best systems simply provide such relations as advice in their
manuals and message boards.

In particular, STAN [[citep:standevelopmentteam_stan_2014]] is well known for
providing a popular manual that advises user-level manipulations to account for
common sampling issues arising from poorly specified models and the deficiencies
of the sampling approach it implements
[[citet:GelmanTransformingparameterssimple2019]].  Among those is the classic
pathological "funnel" model of [[citet:neal_slice_2003]].  This model can be
reparameterized using the following basic relation between a standard Gaussian
random variable and its affine transform:
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:normal-affine-trans}
  \begin{gathered}
    X = \mu + \sigma Y,
    \\
    X \sim \operatorname{N}\mleft( \mu, \sigma^2 \mright),\quad
    Y \sim \operatorname{N}\mleft( 0, 1 \mright),\quad
  \end{gathered}
\end{equation}
#+END_SRC

Under the left-to-right application of Equation [[eqref:eq:normal-affine-trans]],
the funnel model exhibits much better sampling properties.

We would like to see relations like these used within statistical modeling
systems, so that model specification is more flexible and less brittle with
respect to the exact specification of a model.  Systems like the aforementioned
STAN and the Python based PyMC
[[citep:SalvatierProbabilisticprogrammingPython2016]] are also known as
Probabilistic Programming Languages (PPLs), and, as such, their place as
interpreted languages is rather clear.  In line with most programmatically
interpreted languages, compilation can be used to improve performance
and--simultaneously--the expressive potential of the syntax.

Projects like STAN and PyMC rely almost exclusively on AD and have more or less
superseded older projects based on different, non-gradient-based generalized
methodologies, like BUGS [[citep:LunnBUGSprojectEvolution2009]].  BUGS used some
of the domain-specific relations implied here to construct a surprisingly robust
expert-like system that would automatically construct a slice sampler for a
given model.  We would like to see approaches like this enhanced by the symbolic tools
described here *and* the tensor libraries now providing AD to many of these newer
modeling systems.

One noteworthy example would involves PyMC's logic for determining which sampler
is used.  Just like the optimization system in Theano, a the graph of a PyMC model
could be manipulated to produce a more suitable, yet equivalent, model for a given
sampler, or--conversely--produce a customized sampler for a given model.
In extreme cases (an example of which is provide in a later section), the posterior
distribution ultimately estimated by PyMC could be returned in closed-form.

Otherwise, there are entire classes of efficient, model-specific samplers that
are out of these PPLs' reach, and the addition of some straight-forward and
flexible term rewriting capabilities would make them immediately available.
Some examples involve basic Gibbs samplers, scale mixture representations that
lead to efficient samplers for sparsity priors
[[citep:BhadraDefaultBayesiananalysis2016]] and non-Gaussian models
[[citep:polson_bayesian_2013]], and parameter expansions
[[citep:scott_parameter_2010]].

The missing functionality that would drive any automated application of these
methods is essentially falls within the domain of term rewriting
[[citep:BaaderTermrewritingall1999]].  Even the algebraic optimizations
currently employed by modern tensor libraries are fundamentally limited by the
basic choices and challenges covered by term rewriting.  Due to these basic
challenges, the work described above will most likely need more flexibility than
not in the way it navigates the space of applied term rewriting.  Instead of
making bold choices in how term rewriting operations are expressed and
implemented, it will need to avoid a development path that produces little in
the way of reusable work.  It will also need to exist in a form that's at least
recognizable to people working in the related fields of term rewriting; that way,
there's more potential for immediate cross-over.

To reiterate, we envision a light-weight means of establishing collections of
domain-specific relations for interactive and automated use.  These relations
can be created, used, and refined directly by the experts that work with or
develop them and then deployed in sophisticated systems that are jointly
developed by experts in term rewriting, type theory, code synthesis, and related
research.
# Analogous efforts can be seen in projects like the functions grimoire of
# [[citet:Johanssonfungrim2020]].

* Where miniKanren fits in

Computer science researchers have been--and continue to--actively pursue topics
in symbolic computation specifically for the area of statistical modeling
[[citep:Waliahighlevelinferencealgorithms2018,
GorinovaAutomaticReparameterisationProbabilistic,
SatoFormalverificationhigherorder2018, ShanExactBayesianInference2017]].
Unfortunately, much of this work takes the form of entirely new languages and
very specialized high-level semantics that do not lend well to adoption by
experts in the areas of statistical modeling and methods.  This limitation
severely restricts the efficacy of such systems and their implementations by
requiring that they solve notoriously difficult symbolic problems in order to
compete with simple patchworks of software that implement relatively narrow
specialized methods.

Put another way, it's like the two extremes mentioned earlier, where--in this
case--a statistician could specify an efficient slice sampler for a class
of sparsity priors--like the Horseshoe and Horseshoe+ priors
[[citep:BhadraDefaultBayesiananalysis2016]]--or use a generalized symbolic
system that attempts to derive such slice samplers through first principles.
Unfortunately, the latter falls within a set of well known and extremely
difficult problems in symbolic mathematics, AI, and computer science in general
(e.g. automatic theorem proving), and the former leads to a zoo of inflexible
software packages that can rarely be combined and that alone provide only marginal
utility.

Much of the same can be said about work in symbolic computation for
tensor algebra.  In these cases, efforts tend to focus more on new characterizations
of standard term rewriting that are too restrictive in their use and concern for
very specific sets of relations (albeit rarely--if ever--realized as actual
relations), methodologies, and programming languages.  For example, there's
platform-specific rewriting work for automatic conjugation
[[citep:HoffmanAutoconjRecognizingExploiting2018]] in Python that works off
of a specially designed term rewriting mechanism called "interception", neural
network-specific DSLs and frameworks [[citep:WeiDLVMmoderncompiler2017,
VasilacheTensorComprehensionsFrameworkAgnostic2018]], specialized tensor calculi
[[citep:MullinTensorsndArrays2009]], etc.  Given the degree of specialization in
this area of work, few--if any--of the resulting frameworks or implementations
provide an apparent benefit to more than a couple of the objectives stated in
Section [[#sec:want-to-do]].

In general, work that reformulates--or even implicitly obfuscates--the well
established underlying frameworks and mechanics behind the relevant symbolic
computations, like term rewriting and unification, aren't viable without very
clear theoretical, conceptual, and/or implementation advantages.  Essentially,
there should be a good reason for adding conceptual barriers to established
areas of symbolic computation.

Instead, we believe it's more immediately constructive to further the
development of lightweight symbolic tools that are flexible with respect to the
programming language requirements, address the unavoidable and important theoretical
underpinnings (e.g. term rewriting, unification) in a minimally sufficient way,
and easily allow for arbitrary low-level interventions at the implementation
level.  Such a context would be much more conducive to the growth of extensible
logic and code.

This is where miniKanren comes in.  It serves as a minimal, lightweight
relational DSL that orchestrates unification and reification
[[citep:HemannmKanrenminimalfunctional2013]] and operates exclusively within an
existing host language.  As well, it doesn't attempt to reformulate or avoid its
connection to the relevant theoretical concepts or areas of study, so, for
instance, its preexisting use in type theory automatically provides exciting
connections to both basic and cutting-edge symbolic computation (e.g. theorem
proving [[citep:NearalphaleanTAPDeclarativeTheorem2008]]).

miniKanren inherently provides a degree of high-level portability and low-level
flexibility.  Relations can be built on top of other relations, and--dependent
on the degree of host language-specificity--it can be quite clear how
relations can be implemented in other host languages.  This is exactly the type
of generality that provides the basis for a less language-specific
community of applied mathematics and statistics-based optimizations.

Likewise, when performance is absolutely necessary, it's easy to implement
relations directly in the host language.

In the following section, we will illustrate these points
using a Python implementation of miniKanren.

* Symbolic Computation in Python Driven by miniKanren

In the following sections we detail our implementation of miniKanren
[[citep:Willardpythologicalkanren]], operating under the PyPy
name src_python[:eval never]{miniKanren} and Python package
name src_python[:eval never]{kanren}, and its ecosystem of complementary
packages.

src_python[:eval never]{kanren} is a fork that tries to maintain parity with
the original implementation, but now deviates significantly in a number of
ways described in what follows.  Both this fork and the original version
have some small, but noteworthy, differences from the standard miniKanren
literature.  Specifically, the \(\equiv\) goal is represented by the function
src_python[:eval never]{eq}, there is no ~fresh~--instead, fresh logic variables
are constructed explicitly using the function src_python[:eval never]{var}, and
the functionality of ~bind~ and ~mplus~ are represented by the logical "and" and "or"
functions src_python[:eval never]{lall} and src_python[:eval never]{lany} and
are essentially the ~conj~ and ~disj~ of [[citet:HemannmKanrenminimalfunctional2013]].

** Goals as Generators
Our implementation of miniKanren represents goals and goal streams using
Python's built-in generators [[citep:GeneratorsPythonWiki]].  The resulting
simplicity of this approach is an example of how well miniKanren's underlying
mechanics can be adapted to host languages in which the standard list-based
approach isn't as natural or efficient as it is in Scheme.

As Listing [[low-level-relations]] shows, a goal can take full advantage of the
generator capabilities.

#+NAME: low-level-relations
#+BEGIN_SRC python :eval never
def relationo(*args):
    """Construct a goal for this relation."""

    def relationo_goal(S):
        """Generate states for the relation `relationo`.

        I.e. this is the goal that's generated.

        Parameters
        ----------
        S: Mapping
            The miniKanren state (e.g. unification mappings/`dict`).

        Yields
        ------
        miniKanren states.

        """
        nonlocal args

        # Make sure we're working with the state-wise current values of the
        # relation's arguments.
        args_rf = reify(args, S)

        # Now, implement the goal!
        #
        # Remember, a goal succeeds when it `yield`s at least one state (e.g. `S`),
        # and it fails when it `yield`s `None` (or simply `return`s).
        #
        # Just for reference, here are a few common goal objectives and their
        # `yield` idioms.
        #

        # 1. If you want a "stateful" goal, you can do standard Python generator
        # work.
        x = 1
        for a in args_rf:
            S_new = S.copy()
            if isvar(a):
                S_new[a] = x
            yield S_new
            x += 1

        # 2. If you only want to confirm something in/about the state, `S`, then
        # simply `yield` it if the condition(s) are met:
        if some_condition:
            yield S
        else:
            # If the condition isn't met, end the stream by returning/not
            # `yield`ing anything.
            return

        # 3. If you can do everything using existing goal constructors, then
        # simply use `yield from`:
        yield from lall(conso(1, var(), args_rf), ...)

        # 4. If you want to implement a recursive goal, simply call the goal
        # constructor.  It won't recurse endlessly, because this goal
        # needs to be evaluated before the recursive call is made.
        # This is how you create infinite miniKanren streams/results that yield
        # lazily.
        yield from relationo(*new_args)

        # 3. + 4. can be combined with 1. to directly use the results produced
        # from other goal constructor streams and--for example--arbitrarily
        # reorder the output and evaluation of goals.

    # Finally, return the constructed goal.
    return relationo_goal
#+END_SRC

The implementation patterns described in Listing [[low-level-relations]] are
realized in a number of low-level goal implementations
within src_python[:eval never]{kanren}.  One good example is the
goal src_python[:eval never]{permuteo}, which relates an ordered collection to
its permutations.  Within src_python[:eval never]{permuteo}, low-level Python
steps are taken in order to efficiently difference collections using hashes of
their elements and to iteratively compute permutations using Python's
built-in src_python[:eval never]{permutations}.

This approach also makes it possible for goals to more easily control the order
of output results in their generated goal streams, and, using Python's coroutine
capabilities, it's also possible to send results back to a goal producing an
goal stream [[citep:PEP342Coroutines]].

** Constraints
Our Python implementation follows the approach of
[[citet:Hemannframeworkextendingmicrokanren2017]] to implement a minimal
constraint system in miniKanren.

#+NAME: constraints-example
#+BEGIN_SRC python :eval never
>>> from kanren.constraints import neq, isinstanceo

>>> run(0, x,
...     neq(x, 1),  # Not "equal" to 1
...     neq(x, 3),  # Not "equal" to 3
...     membero(x, (1, 2, 3)))
(2,)

>>> from numbers import Integral
>>> run(0, x,
...     isinstanceo(x, Integral),  # `x` must be of type `Integral`
...     membero(x, (1.1, 2, 3.2, 4)))
(2, 4)
#+END_SRC

** ~CONS~
One of the main challenges involved in implementing miniKanren in some host languages is
the lack of immediate support for some important Scheme/Lisp-like elements.  The most notable
being ~CONS~.  Our Python implementation of miniKanren preserves nearly all the
same algebraic datatype semantics of ~CONS~ pairs through
a src_python[:eval never]{cons} package
[[citep:Willardpythologicalpythoncons2020]] that provides an easily extensible
class and set of generic functions that incorporate Python's built-in sequence
types.

~CONS~ support is important for maintaining certain forms of simplicity and
expressiveness in term rewriting.  For instance, while "pattern matching"--or
unification--alone can be rather straight-forward to implement, and more than a
couple of the software systems mentioned here have introduced basic pattern
matching, they all tend to lack simplicity afforded by the use of list-based
terms and proper ~CONS~ semantics, making unification against a term's ~CDR~ or
"tail" cumbersome to express (or implement).

A good example is Theano's unification system
[[citep:GraphoptimizationTheano2020]]; attempting to construct a pattern that
matches a specific ~CAR~, or operator, and an unspecified number/type of
arguments appears to fall outside of the unification system's reach entirely.

As Listing [[cons-cdr-unify-example]] demonstrates,
with src_python[:eval never]{cons} we're able to succinctly express such a
"pattern" very easily for all the built-in ordered collection types.
#+NAME: cons-cdr-unify-example
#+BEGIN_SRC python :eval never
>>> unify([1, 2], cons(var('car'), var('cdr')), {})
{~car: 1, ~cdr: [2]}

>>> reify(cons(1, var('cdr')), {var('cdr'): [2, 3]})
[1, 2, 3]

>>> reify(cons(1, var('cdr')), {var('cdr'): None})
[1]
#+END_SRC

In line with our broader objectives, we believe that sufficiently powerful and
expressive "pattern matching" shouldn't operate outside of the host language and
its familiar types--if it isn't absolutely required.

** S-Expressions

Since Python doesn't provide a programmatically convenient intermediate form of
expressions or terms, we've constructed a
simple src_python[:eval never]{ExpressionTuple} type that extends the
built-in src_python[:eval never]{tuple} with the ability to evaluate itself and
cache its results.  While Python does provide AST objects that fully represent
expressions, they are cumbersome to work with and do not provide much in the way
of relevant functionality (e.g. access to nested elements is too indirect, their
construction and use involves irrelevant meta information, etc.)

#+NAME: etuple-examples
#+BEGIN_SRC python :eval never
>>> from operator import add
>>> from etuples import etuple, etuplize

>>> et = etuple(add, 1, 2)
>>> et
ExpressionTuple((<built-in function add>, 1, 2))
#+END_SRC

These types can be indexed--and generally treated--like immutable src_python[:eval never]{tuple}s:
#+NAME: etuple-index
#+BEGIN_SRC python :eval never
>>> et[0:2]
ExpressionTuple((<built-in function add>, 1))
#+END_SRC

Evaluation is available through a simple cached property:
#+NAME: etuple-eval
#+BEGIN_SRC python :eval never
>>> et.eval_obj
3
#+END_SRC

src_python[:eval never]{etuple}s bridge one of the obvious gaps between
the Lisp-like capabilities common to miniKanren and its world.
Furthermore, it is easy to specify conversions from and
to src_python[:eval never]{etuple}s for arbitrary types.

Listing [[etuple-custom-class]] constructs two custom classes, src_python[:eval never]{Node}
and src_python[:eval never]{Operator}, and specifies the ~CAR~ and ~CDR~ for the
src_python[:eval never]{Node} type via the generic functions
[[citep:Rocklinmultipledispatch2019]]
src_python[:eval never]{rands} src_python[:eval never]{rator}, respectively.
An src_python[:eval never]{apply} dispatch is also specified, which represents a
combination of ~CONS~ (via the aforementioned src_python[:eval never]{cons}
library) and an S-expression evaluation.

#+NAME: etuple-custom-class
#+BEGIN_SRC python :eval never
from collections.abc import Sequence

from etuples import rator, rands, apply
from etuples.core import ExpressionTuple



class Node:
    def __init__(self, rator, rands):
        self.rator, self.rands = rator, rands

    def __eq__(self, other):
        return self.rator == other.rator and self.rands == other.rands


class Operator:
    def __init__(self, op_name):
        self.op_name = op_name

    def __call__(self, *args):
        return Node(Operator(self.op_name), args)

    def __repr__(self):
        return self.op_name

    def __eq__(self, other):
        return self.op_name == other.op_name


rands.add((Node,), lambda x: x.rands)
rator.add((Node,), lambda x: x.rator)


@apply.register(Operator, (Sequence, ExpressionTuple))
def apply_Operator(rator, rands):
    return Node(rator, rands)
#+END_SRC

With the specification of src_python[:eval never]{rands}, src_python[:eval never]{rator},
and src_python[:eval never]{apply} for src_python[:eval never]{Node} types, it is now
possible to convert src_python[:eval never]{Node} objects to src_python[:eval never]{etuples}
using the src_python[:eval never]{etuplize} function.  Listing [[etuple-custom-etupleize]]
demonstrates this process and shows how the underlying object is preserved through
conversion and evaluation.

#+NAME: etuple-custom-etupleize
#+BEGIN_SRC python :eval never
>>> mul_op, add_op = Operator("*"), Operator("+")
>>> mul_node = Node(mul_op, [1, 2])
>>> add_node = Node(add_op, [mul_node, 3])
>>> et = etuplize(add_node)

>>> pprint(et)
e(+, e(*, 1, 2), 3)

>>> et.eval_obj is add_node
True
#+END_SRC

** Relations for Term Rewriting
Our Python implementation of miniKanren is motivated by the need to cover some of the
symbolic computation objectives laid out here, so, in response, it provides relations
that are specific to those needs.

The most important set of relations involve graph traversal and
manipulation.  src_python[:eval never]{symbolic-pymc} provides "meta" relations
for applying goals to arbitrary "walkable" structures (i.e. collections that fully
support src_python[:eval never]{cons} semantics via src_python[:eval never]{car}
and src_python[:eval never]{cdr}).

Listing [[math-reduceo]] constructs an example goal that represents two simple
mathematical identities: i.e. \(x + x = 2 x\) and \(\log \exp x = x\).

#+NAME: math-reduceo
#+BEGIN_SRC python :eval never
def single_math_reduceo(expanded_term, reduced_term):
    """Construct a goal for some simple math reductions."""
    # Create a logic variable to represent our variable term "x"
    x_lv = var()
    return lall(
        # Apply an `isinstance` constraint on the logic variable
        isinstanceo(x_lv, Real),
        isinstanceo(x_lv, ExpressionTuple),
        conde(
            # add(x, x) == mul(2, x)
            [eq(expanded_term, etuple(add, x_lv, x_lv)),
             eq(reduced_term, etuple(mul, 2, x_lv))],
            # log(exp(x)) == x
            [eq(expanded_term, etuple(log, etuple(exp, x_lv))),
             eq(reduced_term, x_lv)]),
    )
#+END_SRC

We can combine the goal in Listing [[math-reduceo]] with the "meta"
goal, src_python[:eval never]{reduceo}, which applies a goal recursively until a
fixed-point is reached--assuming the relevant goal is a reduction, of course.
(The meta goal should really be named src_python[:eval never]{fixedpointo}.)
Additionally, we create another partial function that sets some default arguments
for the src_python[:eval never]{walko} meta goal.

#+NAME: math-partials
#+BEGIN_SRC python :eval never
math_reduceo = partial(reduceo, single_math_reduceo)
term_walko = partial(walko, rator_goal=eq, null_type=ExpressionTuple)
#+END_SRC

Listing [[math-expand-reduce]] applies the goals to two unground logic variables, demonstrating
how miniKanren nicely covers both term expansion and reduction, as well as graph traversal
and fixed-point calculations, in a single concise framework. (The symbols
prefixed by src_python[:eval never]{~_} in the output are unground logic variables.)

#+NAME: math-expand-reduce
#+BEGIN_SRC python :eval never
>>> expanded_term = var()
>>> reduced_term = var()
>>> res = run(10, [expanded_term, reduced_term],
>>>           term_walko(math_reduceo, expanded_term, reduced_term))

>>> rjust = max(map(lambda x: len(str(x[0])), res))
>>> print('\n'.join((f'{str(e):>{rjust}} == {str(r)}' for e, r in res)))
                                        add(~_2291, ~_2291) == mul(2, ~_2291)
                                                   ~_2288() == ~_2288()
                              log(exp(add(~_2297, ~_2297))) == mul(2, ~_2297)
                                ~_2288(add(~_2303, ~_2303)) == ~_2288(mul(2, ~_2303))
                    log(exp(log(exp(add(~_2309, ~_2309))))) == mul(2, ~_2309)
                                             ~_2288(~_2294) == ~_2288(~_2294)
          log(exp(log(exp(log(exp(add(~_2315, ~_2315))))))) == mul(2, ~_2315)
                                           ~_2288(~_2300()) == ~_2288(~_2300())
log(exp(log(exp(log(exp(log(exp(add(~_2325, ~_2325))))))))) == mul(2, ~_2325)
                        ~_2288(~_2294, add(~_2331, ~_2331)) == ~_2288(~_2294, mul(2, ~_2331))
#+END_SRC

To further demonstrate the expressive power of miniKanren in this context, in
Listing [[math-constrained-expand-reduce]] we show how easy it is to perform
term reduction, expansion, or both under structural constraints on the desired
terms.  Specifically, we ask for the first ten expanded/reduced term pairs where
the expanded term is a logarithm with at least one argument.

#+NAME: math-constrained-expand-reduce
#+BEGIN_SRC python :eval never
>>> res = run(10, [expanded_term, reduced_term],
>>>           conso(log, exp_term_cdr, expanded_term),
>>>           conso(var(), var(), exp_term_cdr),
>>>           term_walko(math_reduceo, expanded_term, reduced_term))

>>> rjust = max(map(lambda x: len(str(x[0])), res))
>>> print('\n'.join((f'{str(e):>{rjust}} == {str(r)}' for e, r in res)))
                              log(exp(add(~_2457, ~_2457))) == mul(2, ~_2457)
                                   log(add(~_2467, ~_2467)) == log(mul(2, ~_2467))
                                           log(exp(~_2446)) == ~_2446
                                                log(~_2460) == log(~_2460)
                    log(exp(log(exp(add(~_2477, ~_2477))))) == mul(2, ~_2477)
                                              log(~_2464()) == log(~_2464())
          log(exp(log(exp(log(exp(add(~_2487, ~_2487))))))) == mul(2, ~_2487)
                           log(~_2460, add(~_2493, ~_2493)) == log(~_2460, mul(2, ~_2493))
log(exp(log(exp(log(exp(log(exp(add(~_2499, ~_2499))))))))) == mul(2, ~_2499)
                         log(log(exp(add(~_2501, ~_2501)))) == log(mul(2, ~_2501))
#+END_SRC

** The src_python[:eval never]{symbolic-pymc} Package

In order to bridge miniKanren and the popular tensor libraries Theano and TensorFlow,
the project src_python[:eval never]{symbolic-pymc} provides "meta" type wrappers
for the basic tensor graphs in each library.  These meta types allow for more
graph mutability than the "base" libraries themselves tend to provide.  They also
allow one to use logic variables where the base libraries wouldn't.

Basic use of src_python[:eval never]{symbolic-pymc} involves either conversion
of an existing base--i.e. Theano or TensorFlow--graph into a corresponding meta graph, or
direct construction of meta graphs that are later converted (or "reified") to
one of the base graph types.  miniKanren goals generally work at the meta graph level,
where--for instance--one would apply goals that unify a converted base graph
with a pure meta graph containing logic variables.

While it is possible to achieve the same results with only src_python[:eval never]{etuple}s,
meta graphs are a convenient form that allow developers to think and operate at the
standard Python object level.  They also provide a more direct means of graph
validation and setup, since checks can--and are--performed during meta graph construction,
whereas standard src_python[:eval never]{etuple}s would not be able to perform
such operations until the resulting src_python[:eval never]{etuple} is fully
constructed and evaluated.  Likewise, meta graphs are more appropriate for
specifying and obtaining derived information, like shapes and data types, for a
(sub)graph instead of miniKanren.

To demonstrate the use of src_python[:eval never]{symbolic-pymc}, we consider a
simple conjugate model constructed using PyMC3 in Listing [[beta-binom-setup]].

#+NAME: beta-binom-setup
#+BEGIN_SRC python :eval never
import pymc3 as pm

with pm.Model() as model:
    p = pm.Beta("p", alpha=2, beta=2)
    y = pm.Binomial("y", n=totals, p=p, observed=obs_counts)
#+END_SRC

A user will generally have some data specifying the values
for src_python[:eval never]{totals} and src_python[:eval never]{obs_counts} and
will want to estimate the posterior distribution of src_python[:eval never]{p}.  One way to
phrase this situation: we want to estimate the distribution of a rate of
success, src_python[:eval never]{p}, for an event given a total number of
events, src_python[:eval never]{totals}, and observed
successes, src_python[:eval never]{obs_counts}, under the assumption that
the events are binomially distributed with rate src_python[:eval never]{p}, itself
having a beta prior distribution.

Mathematically, this simple model is stated as follows:
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:beta-binom-model}
  \begin{aligned}
    Y &\sim \operatorname{Binom}\mleft( N, p \mright)
    \\
    p &\sim \operatorname{Beta}\mleft( 2, 2 \mright)
  \end{aligned}
\end{equation}
#+END_SRC

This model has a well known closed-form posterior distribution given by
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:beta-binom-post}
  \left( p \mid Y=y \right) \sim \operatorname{Beta}\mleft( 2 + y, 2 + N - y \mright)
\end{equation}
#+END_SRC

Instead of wasting resources estimating the posterior numerically
(e.g. using src_python[:eval never]{pm.sample(model)} to run a Markov Chain
Monte Carlo sampler), we can simply extract the underlying Theano graph
from src_python[:eval never]{model} and apply a relation that represents the
underlying conjugacy and use the resulting posterior.

Listing [[beta-binom-convert]] converts the PyMC3 model object, src_python[:eval never]{model},
into a standard Theano graph that represents the relationship between random variables
in the model.

#+NAME: beta-binom-convert
#+BEGIN_SRC python :eval never
from symbolic_pymc.theano.pymc3 import model_graph
from symbolic_pymc.theano.utils import canonicalize


# Convert the PyMC3 graph into a symbolic-pymc graph
fgraph = model_graph(model)

# Perform a set of standard algebraic simplifications using Theano
fgraph = canonicalize(fgraph, in_place=False)
#+END_SRC

Listing [[beta-binom-goal]] uses miniKanren to construct a
goal, src_python[:eval never]{betabin_conjugateo}, that matches terms taking the
form of Equation [[eqref:eq:beta-binom-model]] in the first argument and the
resulting posterior of Equation [[eqref:eq:beta-binom-post]] in the second argument.
It makes use of both meta objects and src_python[:eval never]{etuple}s.

#+NAME: beta-binom-goal
#+BEGIN_SRC python :eval never
def betabin_conjugateo(x, y):
    """Replace an observed Beta-Binomial model with an unobserved posterior Beta-Binomial model."""
    obs_lv = var()

    beta_size, beta_rng, beta_name_lv = var(), var(), var()
    alpha_lv, beta_lv = var(), var()
    # We use meta objects directly to construct the "pattern" we want to match
    beta_rv_lv = mt.BetaRV(alpha_lv, beta_lv, size=beta_size, rng=beta_rng, name=beta_name_lv)

    binom_size, binom_rng, binom_name_lv = var(), var(), var()
    N_lv = var()
    binom_lv = mt.BinomialRV(N_lv, beta_rv_lv, size=binom_size, rng=binom_rng, name=binom_name_lv)

    # Here we use etuples for the output terms
    obs_sum = etuple(mt.sum, obs_lv)
    alpha_new = etuple(mt.add, alpha_lv, obs_sum)
    beta_new = etuple(mt.add, beta_lv, etuple(mt.sub, etuple(mt.sum, N_lv), obs_sum))

    beta_post_rv_lv = etuple(
        mt.BetaRV, alpha_new, beta_new, beta_size, beta_rng, name=etuple(add, beta_name_lv, "_post")
    )
    binom_new_lv = etuple(
        mt.BinomialRV,
        N_lv,
        beta_post_rv_lv,
        binom_size,
        binom_rng,
        name=etuple(add, binom_name_lv, "_post"),
    )

    return lall(eq(x, mt.observed(obs_lv, binom_lv)), eq(y, binom_new_lv))

#+END_SRC

Finally, Listing [[beta-binom-run]] shows how the goal can be applied to the model's graph
and how a new Theano graph and PyMC3 model is constructed from the output.

#+NAME: beta-binom-run
#+BEGIN_SRC python :eval never
from symbolic_pymc.theano.pymc3 import graph_model


q = var()
res = run(1, q, betabin_conjugateo(fgraph.outputs[0], q))

expr_graph = res[0].eval_obj
fgraph_conj = expr_graph.reify()

# Convert the Theano graph into a PyMC3 model
model_conjugated = graph_model(fgraph_conj)
#+END_SRC

A more thorough walk-through of src_python[:eval never]{symbolic-pymc} and
miniKanren--using TensorFlow graphs--is given in
[[citet:WillardTourSymbolicPyMC2020]].

:TODO:
  - Hurdles due to no recursion support (e.g. no TCO),
    - Manually designed trampolines to overcome src_python[:eval never]{RecursionError}s
  - Build around/on top of the tensor-libraries-of-the-year while maintaining some portability to the relations (i.e. effectively decouple relations from underlying graph libraries)
  - The need for symbolic DSLs to work for modelerâ€™s and not just the developers
    - Exploratory tools used by the domain expert for research
      - E.g. show me all the ways that a model can be reformulated under mixture representations
  - Relational expression manipulation and its value to statistical modeling
  - Build libraries of the *relations* that underly numeric stability "tricks" (e.g. STAN funnel example), that generate custom samplers for a given model
:END:
* What's needed from/in miniKanren
- In general, for miniKanren to fulfill these purposes, it needs to be combined with some standard applied term rewriting features
- Relational graph traversal
  - Examples of [[https://github.com/pythological/kanren/blob/master/doc/graphs.md][existing relations]]
  - Include and compute graph "metadata" (e.g. tag "operators" for efficient traversal to relevant nodes)
- Efficiently computing fixed-points for relations (e.g. tabling)
- Adding [[https://github.com/pythological/kanren/pull/27][associative-commutative relations]] and general equational logic to
  miniKanren (and perhaps not via extensions to unification?)
- Guided src_python[:eval never]{conde} branching [[citep:SwordsGuidedSearchminiKanren]]
  - Using measures for computational cost and stochastic system "complexity" (e.g. Rao-Blackwellization)
  - E.g. src_python[:eval never]{condp}
- Scaling for large and complex src_scheme[:eval never]{(conde ((== form-in form-in-template) (== form-out form-out-template)))}
  - Some type of goal "compilation"
  - Could be related to, or involve, completion
* Discussion
- New symbolic "platforms" are being developed all the time and implementing
  sophisticated symbolic logic is becoming more challenging and risky, which
  disincentives such efforts
- Statistical modeling and machine learning within miniKanren [[citep:ZhangNeuralGuidedConstraint2018]]
- We need a light-weight suitable platform for symbolic, math-level work that doesn't force us to compromise existing platforms or shoehorn abstractions
- Are there aspects of miniKanren that lend well (or otherwise) to more advanced--and relevant--term rewriting capabilities?
  - Relational completion, completion for non-terminating rewrite systems
- This work is being done across [[https://github.com/pymc-devs/symbolic-pymc][src_python[:eval never]{symbolic-pymc}]] and the [[https://github.com/pythological][Pythological]] packages.


#+BIBLIOGRAPHYSTYLE: plainnat
#+BIBLIOGRAPHY: ../tex/ICFP2020.bib
