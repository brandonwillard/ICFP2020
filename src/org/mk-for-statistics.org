#+TITLE: miniKanren as a Tool for Symbolic Computation in Python
#+AUTHOR: Brandon T. Willard
#+DATE: 2020-05-09
#+EMAIL: brandonwillard@gmail.com

#+STARTUP: hideblocks indent hidestars

#+OPTIONS: author:nil date:t ^:nil toc:nil title:t tex:t d:(not "todo" "logbook" "note" "testing" "notes") html-preamble:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+BEGIN_SRC elisp :eval yes :exports none :results silent
;; Remove the automatically added `amssymb`; it has conflicts with `acmart`.
(setq org-latex-default-packages-alist '())
;; `acmart` wants `\maketitle` *after* the abstract, so we need to prevent
;; org-mode from handling `\maketitle` itself and add it ourself.
(setq org-latex-title-command "")
(add-to-list 'org-latex-classes
             '("acmart" "\\documentclass{acmart}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))

;; We want to add line numbers to listings
(setq org-latex+-tcolorbox-listing-env
  "\\newtcblisting[auto counter,number within=section]{oxtcblisting}[1]{%
\tframe hidden,
\tlisting only,
\tlisting engine=minted,
\tminted options={linenos, numbersep=2mm},
\tbreakable,
\tenhanced,
\ttitle after break={\\raggedleft\\lstlistingname\\ \\thetcbcounter~ -- continued},
\tlisting remove caption=false,
\tarc=0pt,
\touter arc=0pt,
\tboxrule=0pt,
\tcoltitle=black,
\tcolbacktitle=white,
\tcenter title,
\t#1
}")
#+END_SRC

#+SETUPFILE: latex-setup.org

#+LATEX_HEADER: \author{Brandon T. Willard}
#+LATEX_HEADER: \affiliation{\city{Chicago} \state{IL} \country{USA}}
#+LATEX_HEADER: \email{bwillard@uchicago.edu}

#+PROPERTY: header-args :eval never-export :exports both :results output drawer replace
#+PROPERTY: header-args+ :session dlm-optimizations :comments noweb
#+PROPERTY: header-args:python :noweb-sep "\n\n"
#+PROPERTY: header-args:latex :results raw replace :exports results :eval yes

#+BEGIN_abstract
In this article, we give a brief overview of the current state and future
potential of symbolic computation within the Python statistical modeling and
machine learning community.  We detail the use of miniKanren
[[citep:ByrdRelationalProgrammingminiKanren2009]] as an underlying framework for
term rewriting and symbolic mathematics, as well as its ability to orchestrate
the use of existing Python libraries per
[[citet:RocklinMathematicallyinformedlinear2013]].  We also discuss the
relevance and potential of relational programming for implementing more robust,
portable, domain-specific ``math-level'' optimizations--with a slight focus on
Bayesian modeling.  Finally, we describe the work going forward and raise some
questions regarding potential cross-overs between statistical modeling and
programming language theory.
#+END_abstract

#+LATEX: \maketitle
#+LATEX: \acmConference{ICFP 2020}{ACM SIGPLAN International Conference on Functional Programming}{August 23-28}{venue}
#+LATEX: \keywords{minikanren,symbolic-pymc,statistics,relational programming,symbolic computation}

#+BEGIN_SRC latex
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002950.10003648.10003670</concept_id>
       <concept_desc>Mathematics of computing~Probabilistic reasoning algorithms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10003790.10003795</concept_id>
       <concept_desc>Theory of computation~Constraint and logic programming</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10003790.10003798</concept_id>
       <concept_desc>Theory of computation~Equational logic and rewriting</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10003790.10003794</concept_id>
       <concept_desc>Theory of computation~Automated reasoning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Probabilistic reasoning algorithms}
\ccsdesc[500]{Theory of computation~Constraint and logic programming}
\ccsdesc[500]{Theory of computation~Equational logic and rewriting}
\ccsdesc[500]{Theory of computation~Automated reasoning}
#+END_SRC

* Introduction
:PROPERTIES:
:CUSTOM_ID: sec:intro-symbolic-computation
:END:

Throughout, we will focus on two categories of tools within the modern
machine learning, statistics, and data science world:  *Tensor Libraries* and
*Probabilistic Programming Languages* (PPLs).

For our purposes, it's sufficient to say that tensor libraries are the modern
wrappers for standard linear algebra operations--traditionally offered by BLAS
[[citep:BLASBasicLinear2020]] and LAPACK [[citep:LAPACKLinearAlgebra2020]]--with
extensions to handle general arrays, perform tensor operations, and
efficiently compute gradients--usually via automatic differentiation (AD).
These libraries are the main workhorses of deep learning (DL) libraries, and,
because of this strong association, tensor libraries often provide
deep learning-specific functionality (e.g. tools for constructing
DL models, common activation functions, etc.)

:TODO:
- Provide a small code example?
:END:

Probabilistic programming languages are domain-specific languages that aid in
the specification of statistical models and the application of estimation
methods on said models.  Often, PPLs reflects the formal, mathematical
specification of statistical modeling.
In general, PPLs are built on top of libraries that implement composable types
representing elements from probability theory--like statistical distributions and
random variables.  Nowadays, these libraries are increasingly backed by tensor
libraries.

:TODO:
- Provide a small code example?
:END:

There is an appreciable amount of symbolic computation behind the standard Deep
Learning libraries of today, like Theano [[citep:bergstra_theano:_2010]],
TensorFlow [[citep:TensorFlow2020]], and PyTorch [[citep:PyTorch2020]].

At the very least, these libraries provide classes that represent tensor algebra
in graphical form and functions that manipulate graphs.  Furthermore, these
graphs are used to compute derivatives--via Automatic Differentiation (AD),
parallelize or vectorize operations, and, in limited cases, explicitly perform
algebraic simplifications.

Regardless of whether or not you're in a camp that says AD falls well within
standard symbolic mathematics, these libraries are undoubtedly performing
symbolic calculations, and most of them are on a path toward even more symbolic
computation and outright symbolic mathematics.

Theano's optimizations are perhaps the best example of more open-ended symbolic
computation within modern tensor libraries.  It provides an entire subsystem for
incorporating custom ``graph optimizations'' that implements pattern matching,
term substitution, utilizes common sub-expression elimination (CSE), offers
multiple graph traversal strategies and fixed-point operators, etc.  The
optimization system is used to perform graph canonicalization and specialization
through a number of standard matrix algebra identities, and, with these, it is
able to avoid numerous unnecessary numeric calculations and increase the overall
``flexibility'' of function/model specification.

Unfortunately, most other tensor libraries do not offer as much in the way of
user-level Python manipulation of graphs or a graph design and format
that lends well to user-level term rewriting.  In the case of TensorFlow, an
internal graph canonicalization and optimization library
[[citep:LarsenTensorFlowGraphOptimizations2019a]] is being actively developed,
but--at the moment--core work is done exclusively in C++ and the potential for
robust user-defined optimizations isn't clear.

Outside of the aforementioned tensor libraries, other projects approach similar
symbolic-oriented operations--like AD--through function tracing
[[citep:JAX2020]] and Python AST parsing [[citep:tangent2020]], instead of
explicitly constructed computation graphs.  As with most tensor libraries,
these projects aren't particularly focused on supporting generic graph
manipulation or symbolic mathematics.  Nevertheless, they demonstrate another
burgeoning entry-point to general symbolic computation.

At this point in time, there is a fairly well established set of modern
machine learning and statistical modeling libraries that are all fundamentally
built upon the basics of symbolic graphs representing
tensor algebra operations.  They all vary in the degree to which they
implement symbolic mathematics or programmatically encode the underlying
high-level mathematics.  Regardless, the case for more advanced symbolic
computation and mathematics is slowly being made by multiple influential
projects, and there's already enough reason and prior work to start assessing
the directions this could go, and how we might get the most out of it.

Ideally, symbolic work within the machine learning and statistical modeling
community would progress by developing constructively on top of well established
projects, so that quality code can be reused, existing expertise can be
leveraged, and community involvement from domain experts is easily incorporated.

Within the Python community, [[citet:RocklinMathematicallyinformedlinear2013]]
states this sentiment with regards to the intelligent use of optimized linear
algebra functions and the basics of term rewriting.  This work follows the same
principles, but focuses on the areas of statistical modeling and, more
specifically, Bayesian modeling.  Still, in order to reach the area of
statistical modeling, the same foundational properties are required: flexible,
compartmentalized, and open systems that are easily integrated with established
libraries and map well to their high-level abstractions.

Most symbolic mathematics libraries do not have these properties.  Instead, they
require one to work entirely within the limited ecosystems they provide, and, as
a result restrict their application within more advanced--albeit narrower--libraries
that specialize in, say, tensor algebra.  Often, only a small portion of a
symbolic mathematics library's functionality is relevant, but it isn't possible
to use these portions independently.  Worse yet, some libraries are literally
their own programming languages and present an unjustifiable barrier to their
integration within other systems.

* What we want to do in Statistical Modeling
:PROPERTIES:
:CUSTOM_ID: sec:want-to-do
:END:

We believe that the statistical modeling and machine learning communities need a
framework within which they can develop their own high-level, symbolic
"optimizations" specific to the domains of statistical modeling and machine
learning.  Ideally, such a framework would have the properties outlined in
Section [[#sec:intro-symbolic-computation]] and build upon the already well
established Python machine learning and statistics ecosystem, instead of
attempting to outright reinvent it or rewrite its staple offerings.

These symbolic optimizations should be usable internally by new and existing
libraries to drive advanced automations.  As well, they should be usable in an
*interactive* way by assisting with the automation of the math-level work performed
during methods research.  In the latter capacity, the framework we describe is
more akin to the standard symbolic mathematics offerings; however, this
consideration should only emphasize the degree to which the framework we desire
is expected to integrate with existing libraries, since our concept of
integration applies equally well to existing symbolic mathematics libraries.

Let's consider statistical modeling explicitly.

Statistical modeling is surprisingly amenable to symbolic methods
[[citep:CaretteModelmanipulationpart2007,
CaretteSimplifyingProbabilisticPrograms2016]]--and especially when one
restricts the context to specific practices like Bayesian modeling
[[citep:ShanExactBayesianInference2017]], where there exist fundamental
relations--such as "conjugacy" [[citep:robert_bayesian_2007][Chapter 3.3]]--that are
easily automated with simple pattern matching.

Regarding widely applicable principles in statistics, one of the most basic is
Rao-Blackwellization.  Rao-Blackwellization is derived from the Rao-Blackwell
Theorem [[citep:CasellaRaoBlackwellisationsamplingschemes1996]], which
states--roughly--that analytically computed conditional expectations outperform
their un-integrated counterparts.  In other words, if you can get a closed-form
answer to an integral, instead of estimating the integral with samples, you
should use the closed-form answer.

It applies in a rather general sense to numerous Markov Chain Monte Carlo
methods, but its automation is something that falls well outside of most
libraries due to its symbolic computation requirements.

# For instance, if one wanted to apply as much Rao-Blackwellization as possible,
# they might have to employ a state-of-the-art symbolic integration routine over
# arbitrary integrals implied by the applicable statistical models.  At the level
# of (nearly) arbitrary integrals, this problem is notoriously difficult.

# Alternatively, with modern symbolic algebra libraries, probability theory features
# are available that allow one to state the same integral problems in a way that's
# more direct to a statistical modeling library that might want to solve such problems.
#
# In this case, the problem is more about the barriers between the two platforms.
# For example, a Python based statistical modeling library would have to perform
# a considerable amount of work to convert its internal model representations
# into a form suitable for an industry-standard symbolic math library like Mathematica
# and vice versa.
#
# Even if the inter-operation were made easier by special bindings and so forth,
# large external dependencies aren't often viable for small, specialized libraries
# that want to serve a wide user-base.  Also, these kinds of dependencies can
# involve considerable context/language shifts that make adding new identities
# too difficult.
#
# Instead, if it were easy to specify high-level relationships symbolically
# without the dramatic context switching or costly dependencies, then more
# practitioners and domain experts might be willing to implement them.  Over time,
# identities and theorems for specific classes of models and distributions would
# be implemented by the researchers working on them, and those results
# could be immediately utilized by numeric libraries.  All the while, the
# coverage of these results could be simultaneously extended by other researchers
# developing higher-level symbolic methods.
#
# Even before this ideal culmination, there's room for something between the
# extremes of enumerating explicit simplifications for each and every possible
# model and stating extremely general axioms from which said simplifications can
# be derived.  We would like to constructively navigate between those two
# extremes.

For instance, Rao-Blackwellizations appearing in published material is often
driven by high-level *relations*.  Some of those relations involve basic theorems
in probability theory like the following identity--expressed as a rule--regarding
the sum of Gaussian random variables:
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:sum-of-normals}
  \begin{aligned}
    \left(\text{sum-of-normals}\right) & \quad
    \frac{
      X \sim \operatorname{N}\mleft(\mu_x, \sigma_x^2\mright), \quad
      Y \sim \operatorname{N}\mleft(\mu_y, \sigma_y^2\mright), \quad
      X + Y = Z
    }{
      Z \sim \operatorname{N}\mleft(\mu_x + \mu_y, \sigma_x^2 + \sigma_y^2\mright)
    }
  \end{aligned}
\end{equation}
#+END_SRC

There are numerous examples like [[eqref:eq:sum-of-normals]], and they all take the
form of relations.  Hiding behind these relations are the closed-form integrals
that would otherwise be painstaking to compute directly with symbolic algebra.

This general idea has analogous in the approaches used by modern symbolic
integration systems themselves.  When such systems employ Fox H and Meijer G
functions [[citep:peasgood_method_2009, roach_meijer_1997]], they are
effectively using only a few simple algebraic convolution relations applied to
broad classes of hypergeometric functions--many of which can be encoded by
simple look-up tables.

Since those systems are intended to reach a much greater number of functions and
constraints, they are necessarily more complex; however, a majority of the work
being done by statistical modeling deals with a comparatively smaller set of
standard distributions, so major improvements can be made without invoking the
complexity of symbolic integration systems.

Currently, statistical modeling systems do not directly support these types of
``knowledge'' additions, nor do they attempt to employ these well-known general
optimizations like Rao-Blackwellization.  Instead, this kind of work is still
restricted to the user-level, where it is performed by hand and used as input to
such systems.  At the present, the best systems simply provide broadly useful
identities and theorems as advice in their manuals and message boards.

In particular, STAN [[citep:standevelopmentteam_stan_2014]] is known for
having a well written manual that details user-level manipulations to account for
common sampling issues arising from poorly specified models
[[citep:GelmanTransformingparameterssimple2019]].  Note that, the description
"poorly specified" is conditional on the given estimation approach.

Among the advice given in STAN's manual is the classic pathological "funnel"
model of [[citet:neal_slice_2003]].  This model can be reparameterized using the
following rule between a standard Gaussian random variable and its
affine transform:
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:normal-affine-trans}
  \begin{aligned}
    \left( \text{normal-affine-transform} \right) &\quad
    \frac{
      Y \sim \operatorname{N}\mleft( 0, 1 \mright), \quad
      X = \mu + \sigma Y
    }{
      X \sim \operatorname{N}\mleft( \mu, \sigma^2 \mright)
    }
  \end{aligned}
\end{equation}
#+END_SRC

Under [[eqref:eq:normal-affine-trans]], terms in the funnel model can be expanded
resulting in an equivalent model that exhibits much better sampling properties.

The work we detail here is motivated by the desire to see relations like these
used within statistical modeling systems, so that model specification is more
flexible and less brittle with respect to the exact specification of a model.

Systems like STAN and the Python-based PyMC
[[citep:SalvatierProbabilisticprogrammingPython2016]] are PPLs, so their role as
programming languages is clear, and--in line with most programming
languages--compiler optimizations can be used to improve performance
and expand the expressive potential of a language's syntax.

Projects like STAN and PyMC rely almost exclusively on AD and have more or less
superseded older projects based on different, non-gradient-based generalized
methodologies, like BUGS [[citep:LunnBUGSprojectEvolution2009]].  BUGS used some
of the domain-specific identities implied here to construct a surprisingly robust
expert system that could automatically construct a sampler for a
given model.  We would like to make such systems easier to produce and extend,
and we would like to see them built on top of modern tensor libraries, so that
the AD-driven methods of modern PPLs can be used in tandem.

One noteworthy example is PyMC's internal logic for determining an appropriate
sampler.  This logic could benefit from an easily extensible, expert-like system
that matches models to samplers.  Just like the optimization system in Theano,
the graph of a PyMC model can be manipulated to produce a more suitable, yet
equivalent, model for a given sampler, or--conversely--produce a customized
sampler for a given model.  In extreme cases, the posterior distribution
ultimately estimated by PyMC could be returned in closed-form.  A small example
of this is given in Section [[#sec:symbolic-pymc]].

Otherwise, there are entire classes of efficient, model-specific samplers that
are currently out of these PPLs' reach, and the addition of some straight-forward and
flexible term rewriting capabilities would make them immediately available.
Some examples involve Gibbs samplers, scale mixture representations for sparsity priors
[[citep:BhadraDefaultBayesiananalysis2016]] non-Gaussian models
[[citep:polson_bayesian_2013]], and parameter expansions
[[citep:scott_parameter_2010]].

As a proof of concept using Theano's existing optimization system, automatic
simplification of random variables was demonstrated in
[[citet:WillardRoleSymbolicComputation2017]].  While it is more than possible to
extend the same approach into auto-conjugation and related statistical
optimizations, the scalability and means of specifying new optimizations within
Theano wasn't promising.

One important concern involves the need to use identities in more than one
direction.  For instance, one direction of the identity underlying
[[eqref:eq:normal-affine-trans]] is useful for computational reasons (e.g.  the
funnel problems) and the other direction helps one determine the distribution
type of a sub-term (i.e. given \(Y \sim \operatorname{N}\mleft( 0, 1 \mright)\)
we can derive distribution of \(X\)).  The latter information might be needed by
a system that construct custom samplers, or to reformulate a model so that it
can be used by a given sampling routine.

This otherwise natural use of identities isn't covered well by modern
programming frameworks, and that's where logic and relational programming
becomes a real consideration.

Considerations like these also lead quickly into the domain of term rewriting
[[citep:BaaderTermrewritingall1999]]. Graph normalization/canonicalization,
rewrite rule completion [[citep:Huetcompleteproofcorrectness1981]], and general
equational reasoning are all ground-level subjects in this situation.

With this in mind, it's likely that our objectives won't often lead to the
classical term rewriting niceties, like easily determined normal forms and term
orderings with strong guarantees.  Given our desire for an interactive system in
which to perform ad hoc additions and experimentation, it seems even less
likely.  Even so, when such niceties are available, we would at least like the
means to derive and apply them--or at framework that also caters to work in
those areas.  Furthermore, if it's ever possible to produce any results in the
worst of term rewriting situations (e.g. amidst non-terminating
operations/systems), then we would like a framework that facilitates that.

Overall, we seek a middle ground that provides an approachable, powerful, yet
light-weight framework for orchestrating domain-specific relations.
These relations could come from identities in statistics and probability theory,
and it should be possible for domain experts to specify them in the programming
languages of their fields.

As well, we would like to see this framework easily deployed in sophisticated
systems, with these systems benefiting from joint development between experts in
term rewriting, type theory, code synthesis, and related areas.  We believe
miniKanren could serve an important role within this intersection of
requirements.

* Where miniKanren fits in

Computer science researchers have been--and continue to--actively pursue topics
in symbolic computation specifically within the area of statistical modeling
[[citep:Waliahighlevelinferencealgorithms2018,
SatoFormalverificationhigherorder2018, ShanExactBayesianInference2017]].
While very in-line with the automations described here, much of this work takes
the form of entirely new languages or very broad theoretical work that doesn't
always lend itself to more immediate input from experts in the areas of
statistical modeling methods.

In other cases, the limitations involve the degree of specialization, where
exclusive focus is often on neural network-specific DSLs and frameworks, or
only certain types of optimizations [[citep:WeiDLVMmoderncompiler2017,
VasilacheTensorComprehensionsFrameworkAgnostic2018]].

Regarding Python and statistical modeling, the recent automatic conjugation and
rescaling examples of [[citet:HoffmanAutoconjRecognizingExploiting2018,
GorinovaAutomaticReparameterisationProbabilistic2018]] are perhaps the most germane;
however, their approach relies entirely on an existing pattern-matching and
rewrite system [[citep:RadulRules2020]] that is non-relational and uses the
Python stack for backtracking.  As we've stated earlier, the use of relations
has important conceptual and implementation advantages (e.g. the concepts being
implemented are fundamentally relational, and the inherent code reuse arising
from "bidirectional" applications of identities).

As well, use of the Python stack for backtracking puts severe limitations on the
size of graphs manageable by such a system.  Python
throws src_python[:eval never]{RecursionErrors} when the stack reaches a fixed
size, and term graphs representing real models are by no means small, so
unification alone is liable to cause irreconcilable errors.  Our implementation
of unification in Python [[citep:Willardlogicalunification2020]] demonstrates
this exact problem in its unit tests, and, as a result, the library uses a
coroutine-based trampoline to avoid excessive use of the Python stack.

Simply increasing the recursion limit (e.g.
via src_python[:eval never]{sys.setrecursionlimit}) is--at best--a single-case
solution, and it's often safer to pursue a more "pythonic" rewrite (i.e. loop or
list comprehension-based approach).  Also, Python currently lacks even the most basic
forms of tail recursion elimination--and it's very unlikely to appear in later
versions [[citep:RossumNeopythonicTailRecursion2009a]].

Furthermore, [[citet:HoffmanAutoconjRecognizingExploiting2018]] doesn't provide
clear examples of how rewrite rules are specified in their proposed system, so
it's difficult to assess exactly how expressive their DSL is, or even how well it
works within Python and its standard collection types.

This is where miniKanren comes in.  It serves as a minimal, lightweight
relational DSL that orchestrates unification and reification and operates
exclusively within an existing host language.  Furthermore, its core
functionality is succinctly described in a single page of code, which helps make
its inner workings very transparent to the interested developer
[[citep:HemannmKanrenminimalfunctional2013]].

In contrast with other unification-driven systems, its "internal" mechanics maintain direct
connections with multiple high-level theoretical concepts (e.g. unification,
complete search, relational programming, CPS, etc.), so, for--instance--its use
as a type theory prototyping language automatically provides exciting connections to
both basic and cutting-edge symbolic computation (e.g. automatic theorem proving
[[citep:NearalphaleanTAPDeclarativeTheorem2008]]).  As a matter of fact, the use
of typing rules to describe the automation of high-level inference algorithms in
[[citet:Waliahighlevelinferencealgorithms2018]] is a direct example of how
elements of type theory can be used in high-level statistical model optimization, and
miniKanren can serve as a bridge to fast implementations.

miniKanren inherently provides a degree of high-level portability and low-level
flexibility.  Relations can be built on top of other relations, and, in these
cases, the goals that implement such composite relations in miniKanren are often
easily ported to miniKanrens in other host languages.  miniKanren doesn't
enforce a formal, host-language independent semantics, yet it still lends well
to this kind of portability.  This lack of formal semantics also makes it easier
to address performance and domain specific issues in multiple ways--like
the src_python[:eval never]{RecursionError}s described above.
With these properties, miniKanren has exactly the type of generality and
flexibility to serve as a basis for more fluid collaboration--in symbolic
computation--between independent communities of computer scientists and
statisticians.

In the following section, we will illustrate many of these points using our
Python implementation of miniKanren.

* Symbolic Computation in Python Driven by miniKanren

In the following sections we detail our implementation of miniKanren
[[citep:Willardpythologicalkanren2020]], operating under the PyPy
name src_python[:eval never]{miniKanren} and Python package
name src_python[:eval never]{kanren}, and its ecosystem of complementary
packages.

src_python[:eval never]{kanren} is a fork that tries to maintain syntactic
parity with its predecessor src_python[:eval never]{logpy}
[[citep:RocklinlogpyLogicProgramming2018]], but now deviates significantly in terms
of core mechanics and offerings.  The most important difference is in the
relational status of src_python[:eval never]{logpy}'s goals; most were not truly
relational.  This was largely due to the use of
an exception-based goal reordering system, which served as the exclusive
means of handling missing src_scheme[:eval never]{cons}-based capabilities and minimalistic constraints.
Basically, one could attempt to develop entirely in standard Python at the goal
constructor level, and throw special src_python[:eval never]{EarlyGoalError}
exceptions when goal constructor arguments were not sufficiently ground for
a given task.  The exception would cause the goals to be reordered until
an ordering satisfied these goals' groundedness requirements.

This exception-based approach was combined with a lightweight tuple-based
Lisp-like expression evaluator that operated in tandem with the goal reordering.  Both
of these components were built directly into the core stream processing
functions and introduced additional complexity and challenges, but, most of all,
they made it much easier to construct non-relational goals and imposed new,
non-miniKanren semantics, that increased the barrier to entry beyond a simple
understanding of core Python and miniKanren.

Our implementation of miniKanren's core mechanics does not operate on Lisp-like
idioms or a its invent its own goal processing semantics.  Additionally, it provides
a straight-forward object-oriented framework for implementing truly relational
constraints.  These points will be covered in more detail in the following
sub-sections.

First, we must note that both Python implementations share the same small,
but noteworthy, deviations from the standard Scheme-based miniKanrens.
Specifically, the \(\equiv\) goal is represented by the
function src_python[:eval never]{eq}, there is no ~fresh~--instead, fresh logic
variables are constructed explicitly using the
function src_python[:eval never]{var}, and the functionality of ~bind~ and
~mplus~ are represented by the logical ``and'' and ``or''
functions src_python[:eval never]{lall} and src_python[:eval never]{lany} and
are essentially the ~conj~ and ~disj~ stream functions of
[[citet:HemannmKanrenminimalfunctional2013]].

** Goals as Generators
Our implementation of miniKanren represents goals using
Python's built-in generators [[citep:GeneratorsPythonWiki2020,
PythonSoftwareFoundationExpressionsPythondocumentation2020]].

Listing [[low-level-relations]] illustrates the general form of a miniKanren goal
and some of the idioms available to them.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Example idioms for generator-based goals in Python.
#+NAME: low-level-relations
#+BEGIN_SRC python :eval never -n 0 -r
def relationo(*args):
    """Construct a goal for this relation."""

    def relationo_goal(S):
        """Generate states for the relation `relationo`.

        I.e. this is the goal that's generated.

        Parameters
        ----------
        S: Mapping
            The miniKanren state (e.g. unification mappings/`dict`).

        Yields
        ------
        miniKanren states.

        """
        nonlocal args

        args_rf = reify(args, S)

        x = 1
        for a in args_rf:
            S_new = S.copy()

            if isvar(a) or x > 3:
                S_new[a] = x

            # If the stream manipulating functions support it, information can
            # be sent back to the goal and used!
            z = yield S_new  # (ref:goal-generator-loop)

            if not z:
                x += 1

        # 2. If you only want to confirm something in/about the state, `S`, then
        # simply `yield` it if the condition(s) are met:
        if some_condition:
            yield S  # (ref:goal-generator-condition)
        else:
            # If the condition isn't met, end the stream by returning/not
            # `yield`ing anything.
            return

        # 3. If you can do everything using existing goal constructors, then
        # simply use `yield from`:
        yield from lall(conso(1, var(), args_rf), ...)  # (ref:goal-generator-stream)

        # 4. If you want to implement a recursive goal, simply call the goal
        # constructor.  It won't recurse endlessly, because this goal
        # needs to be evaluated before the recursive call is made.
        # This is how you create infinite miniKanren streams/results that yield
        # lazily.
        yield from relationo(*new_args)  # (ref:goal-generator-relation)

        # 3. + 4. can be combined with 1. to directly use the results produced
        # from other goal constructor streams and--for example--arbitrarily
        # reorder the output and evaluation of goals.

    # Finally, return the constructed goal.
    return relationo_goal
#+END_SRC

Simply put, a goal is responsible for either explicitly generating its goal
stream (e.g. [[(goal-generator-loop)]], [[(goal-generator-condition)]]) or deferring to
other goals (e.g. [[(goal-generator-relation)]]) and/or goal combinations via the
stream manipulation functions src_python[:eval never]{lall}
and src_python[:eval never]{lany} (e.g. [[(goal-generator-stream)]]).

The idioms described in Listing [[low-level-relations]] are realized in a number of
low-level goal implementations within src_python[:eval never]{kanren}.  One good
example is the src_python[:eval never]{permuteo} goal, which relates an ordered
collection to its permutations.  Within src_python[:eval never]{permuteo},
low-level Python steps are taken in order to efficiently compute differences of
hashable collections when the arguments are ground, and, when one argument
is unground, Python's built-in permutation
generator src_python[:eval never]{itertools.permutations} is used to efficiently
generate unification arguments for the unground term.  This strictly
Python-based low-level implementation of a goal is both completely relational
and considerably more scalable than an implementation built on the basic
miniKanren relations and amounting to Bogosort
[[citep:KiselyovBacktrackinginterleavingterminating2005]].

Within ordinary goals like src_python[:eval never]{relationo_goal} one is able
to leverage the naturally delayed nature of Python's generators and seamlessly
define recursive goals by calls to the outer goal
constructor src_python[:eval never]{relationo}, and, in the case of goal
constructors that do not define their own low-level goals, recursion is
facilitated by the \(\eta\)-delay function, src_python[:eval never]{Zzz}, of
[[citet:HemannmKanrenminimalfunctional2013]].

This approach also makes it possible for goals to more easily control the type
and order of the results it streams.  The loop around Line [[(goal-generator-loop)]]
in Listing [[low-level-relations]], demonstrates how a goal can easily keep and
manage its state--e.g. the variable src_python[:eval never]{x}--and use it to affect
the goal stream it produces.

Also, using Python's coroutine capabilities, Line [[(goal-generator-loop)]] shows
how it's possible to send results back to a goal when the process evaluating the
stream uses src_python[:eval never]{generator.send} [[citep:PEP342Coroutines2020]].
In this case, a goal could be given ``upstream'' information.

Also, using Python's src_python[:eval never]{__length_hint__}
[[citep:PEP424method2020]] spec, goals and stream manipulation functions could be
told when a stream is empty or simply make decisions based on partial information about a
stream's size.  Such information could help determine efficient orderings
within src_python[:eval never]{lall} conjunctions and
between src_scheme[:eval never]{conde} branches, by--say--allowing these
operators to choose finite streams over potentially infinite ones in certain cases.

Overall, the resulting simplicity of this approach is an example of how well
miniKanren's underlying mechanics can be adapted to host languages in which the
standard list-based approach isn't as natural or efficient as it is in Scheme.

** Constraints
Our Python implementation follows the approach of
[[citet:Hemannframeworkextendingmicrokanren2017]] to implement a minimal
constraint system in miniKanren.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Basic constraint goals example.
#+NAME: constraints-example
#+BEGIN_SRC python :eval never
>>> from kanren.constraints import neq, isinstanceo

>>> run(0, x,
...     neq(x, 1),  # Not "equal" to 1
...     neq(x, 3),  # Not "equal" to 3
...     membero(x, (1, 2, 3)))
(2,)

>>> from numbers import Integral
>>> run(0, x,
...     isinstanceo(x, Integral),  # `x` must be of type `Integral`
...     membero(x, (1.1, 2, 3.2, 4)))
(2, 4)
#+END_SRC

** src_scheme[:eval never]{cons}
One of the main challenges involved in implementing miniKanren in some host
languages is the lack of immediate support for important Scheme/Lisp-like
elements.  The most notable for Python being src_scheme[:eval never]{cons}.

src_scheme[:eval never]{cons} support is important for maintaining certain forms
of simplicity and expressiveness in term rewriting.  For instance, while
``pattern matching''--or unification--alone can be rather straight-forward to
implement, and more than a couple of the software systems mentioned here have
introduced basic pattern matching, they all tend to lack the expressive
simplicity afforded by list-based terms and proper src_scheme[:eval never]{cons}
semantics.

A good example is Theano's unification system
[[citep:GraphoptimizationTheano2020]]; although it does provide a tuple-based
interface for defining forms to match and replace, and it supports logic variables
within said forms, it doesn't provide a means of expressing
a src_scheme[:eval never]{cons} pair.  As a result, attempting to construct a
pattern that matches a specific operator (or src_scheme[:eval never]{car}) and
an unspecified number/type of arguments (or src_scheme[:eval never]{cdr})--or
vice versa--falls outside of the system's reach.

Our Python implementation of miniKanren preserves nearly all the same algebraic
datatype semantics of Lisp's src_scheme[:eval never]{cons} by way of
our src_python[:eval never]{cons} package
[[citep:Willardpythologicalpythoncons2020]].  The src_python[:eval never]{cons}
package provides a minimal src_python[:eval never]{ConsType} class, along with a
set of easily extensible generic functions for src_python[:eval never]{car}
and src_python[:eval never]{cdr}.

As Listing [[cons-cdr-unify-example]] demonstrates,
with src_python[:eval never]{cons} we're able to succinctly express such a
``pattern'' for all the built-in ordered collection types.
#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: src_scheme[:eval never]{cons} pair unification and reification using Python's built-in lists.
#+NAME: cons-cdr-unify-example
#+BEGIN_SRC python :eval never
>>> unify([1, 2], cons(var('car'), var('cdr')), {})
{~car: 1, ~cdr: [2]}

>>> reify(cons(1, var('cdr')), {var('cdr'): [2, 3]})
[1, 2, 3]

>>> reify(cons(1, var('cdr')), {var('cdr'): None})
[1]
#+END_SRC

Later, in Listing [[math-constrained-expand-reduce]], we provide another example
of how a src_scheme[:eval never]{cons}-compliant unification system makes
non-trivial patterns easier to express.

** S-Expressions

Since Python doesn't already provide a programmatically convenient form of
expressions or terms, we've constructed a
simple
src_python[:eval never]{ExpressionTuple} class--or src_python[:eval never]{etuple}
for short--that extends the built-in src_python[:eval never]{tuple} with
the ability to evaluate itself, cache the results, and maintain the cached
results between non-modifying reconstructions and re-evaluations of the
same src_python[:eval never]{etuple}.

Python does provide AST objects that fully represent its built-in expressions,
but they are cumbersome to work with and do not provide much of the desired
functionality for term rewriting (e.g. access to nested elements is too
indirect, their construction and use involves irrelevant meta information, etc.)
See [[citet:WillardReadableStringsRelational2018b]] for examples of term
rewriting using Python AST objects and miniKanren.

As we demonstrate in a later example (i.e. Listing
[[math-reduceo]]), src_python[:eval never]{etuples} are an extremely convenient way
to leverage a target library's user-level functions (e.g. TensorFlow's matrix
multiplication function) without having to manually construct fully reifiable
term graphs--many of which require detailed information that may not be
available at the time of a goal's evaluation.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Constructing a simple src_python[:eval never]{etuple}.
#+NAME: etuple-examples
#+BEGIN_SRC python :eval never
>>> from operator import add
>>> from etuples import etuple, etuplize

>>> et = etuple(add, 1, 2)
>>> et
ExpressionTuple((<built-in function add>, 1, 2))
#+END_SRC

src_python[:eval never]{etuples} can be indexed--and generally treated--like
immutable src_python[:eval never]{tuple}s:
#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: src_python[:eval never]{etuple} indexing example.
#+NAME: etuple-index
#+BEGIN_SRC python :eval never
>>> et[0:2]
ExpressionTuple((<built-in function add>, 1))
#+END_SRC

Evaluation is available through a simple cached property:
#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: src_python[:eval never]{etuple} evaluation example.
#+NAME: etuple-eval
#+BEGIN_SRC python :eval never
>>> et.eval_obj
3
#+END_SRC

Furthermore, it is easy to specify conversions to and
from src_python[:eval never]{etuple}s for arbitrary types.

Listing [[etuple-custom-class]] constructs two custom
classes, src_python[:eval never]{Node} and src_python[:eval never]{Operator},
and specifies the src_scheme[:eval never]{car} and src_scheme[:eval never]{cdr}
for the src_python[:eval never]{Node} type via the generic functions
[[citep:Rocklinmultipledispatch2019]]
src_python[:eval never]{rands} src_python[:eval never]{rator}, respectively.
An src_python[:eval never]{apply} dispatch is also specified, which represents a
combination of src_scheme[:eval never]{cons} (via the aforementioned src_python[:eval never]{cons}
library) and an S-expression evaluation.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Adding src_python[:eval never]{etuple} support to a standard Python class.
#+NAME: etuple-custom-class
#+BEGIN_SRC python :eval never
from collections.abc import Sequence

from etuples import rator, rands, apply
from etuples.core import ExpressionTuple


class Node:
    def __init__(self, rator, rands):
        self.rator, self.rands = rator, rands

    def __eq__(self, other):
        return self.rator == other.rator and self.rands == other.rands


class Operator:
    def __init__(self, op_name):
        self.op_name = op_name

    def __call__(self, *args):
        return Node(Operator(self.op_name), args)

    def __repr__(self):
        return self.op_name

    def __eq__(self, other):
        return self.op_name == other.op_name


rands.add((Node,), lambda x: x.rands)
rator.add((Node,), lambda x: x.rator)


@apply.register(Operator, (Sequence, ExpressionTuple))
def apply_Operator(rator, rands):
    return Node(rator, rands)
#+END_SRC

With the specification of src_python[:eval never]{rands}, src_python[:eval never]{rator},
and src_python[:eval never]{apply} for src_python[:eval never]{Node} types, it is now
possible to convert src_python[:eval never]{Node} objects to src_python[:eval never]{etuples}
using the src_python[:eval never]{etuplize} function.  Listing [[etuple-custom-etuplize]]
demonstrates this process and shows how the underlying object is preserved through
conversion and evaluation.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Converting a supported class instance into an src_python[:eval never]{etuple}.
#+NAME: etuple-custom-etuplize
#+BEGIN_SRC python :eval never
>>> mul_op, add_op = Operator("*"), Operator("+")
>>> mul_node = Node(mul_op, [1, 2])
>>> add_node = Node(add_op, [mul_node, 3])
>>> et = etuplize(add_node)

>>> pprint(et)
e(+, e(*, 1, 2), 3)

>>> et.eval_obj is add_node
True
#+END_SRC

** Relations for Term Rewriting
Our Python implementation of miniKanren is motivated by the need to cover some of the
symbolic computation objectives laid out here, so, in response, it provides relations
that are specific to those needs.

The most important set of relations involve graph traversal and
manipulation.  src_python[:eval never]{symbolic-pymc} provides ``meta'' relations
for applying goals to arbitrary ``walkable'' structures (i.e. collections that fully
support src_python[:eval never]{cons} semantics via src_python[:eval never]{car}
and src_python[:eval never]{cdr}).

Listing [[math-reduceo]] constructs an example goal that represents two simple
mathematical identities: i.e. \(x + x = 2 x\) and \(\log \exp x = x\).

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: An example goal that implements some basic mathematical relations.
#+NAME: math-reduceo
#+BEGIN_SRC python :eval never
def single_math_reduceo(expanded_term, reduced_term):
    """Construct a goal for some simple math reductions."""
    # Create a logic variable to represent our variable term "x"
    x_lv = var()
    return lall(
        # Apply an `isinstance` constraint on the logic variable
        isinstanceo(x_lv, Real),
        isinstanceo(x_lv, ExpressionTuple),
        conde(
            # add(x, x) == mul(2, x)
            [eq(expanded_term, etuple(add, x_lv, x_lv)),
             eq(reduced_term, etuple(mul, 2, x_lv))],
            # log(exp(x)) == x
            [eq(expanded_term, etuple(log, etuple(exp, x_lv))),
             eq(reduced_term, x_lv)]),
    )
#+END_SRC

We can combine the goal in Listing [[math-reduceo]] with the ``meta''
goal, src_python[:eval never]{reduceo}, which applies a goal recursively until a
fixed-point is reached--assuming the relevant goal is a reduction, of course.
(The meta goal should really be named src_python[:eval never]{fixedpointo}.)
Additionally, we create another partial function that sets some default arguments
for the src_python[:eval never]{walko} meta goal.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Partial functions for a fixed-point calculation and graph walking.
#+NAME: math-partials
#+BEGIN_SRC python :eval never
math_reduceo = partial(reduceo, single_math_reduceo)
term_walko = partial(walko, rator_goal=eq, null_type=ExpressionTuple)
#+END_SRC

Listing [[math-expand-reduce]] applies the goals to two unground logic variables, demonstrating
how miniKanren nicely covers both term expansion and reduction, as well as graph traversal
and fixed-point calculations, in a single concise framework. (The symbols
prefixed by src_python[:eval never]{~_} in the output are unground logic variables.)

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Simultaneous mathematical term ``expansion'' and ``reduction''.
#+NAME: math-expand-reduce
#+BEGIN_SRC python :eval never
>>> expanded_term = var()
>>> reduced_term = var()
>>> res = run(10, [expanded_term, reduced_term],
>>>           term_walko(math_reduceo, expanded_term, reduced_term))

>>> rjust = max(map(lambda x: len(str(x[0])), res))
>>> print('\n'.join((f'{str(e):>{rjust}} == {str(r)}' for e, r in res)))
                                        add(~_2291, ~_2291) == mul(2, ~_2291)
                                                   ~_2288() == ~_2288()
                              log(exp(add(~_2297, ~_2297))) == mul(2, ~_2297)
                                ~_2288(add(~_2303, ~_2303)) == ~_2288(mul(2, ~_2303))
                    log(exp(log(exp(add(~_2309, ~_2309))))) == mul(2, ~_2309)
                                             ~_2288(~_2294) == ~_2288(~_2294)
          log(exp(log(exp(log(exp(add(~_2315, ~_2315))))))) == mul(2, ~_2315)
                                           ~_2288(~_2300()) == ~_2288(~_2300())
log(exp(log(exp(log(exp(log(exp(add(~_2325, ~_2325))))))))) == mul(2, ~_2325)
                        ~_2288(~_2294, add(~_2331, ~_2331)) == ~_2288(~_2294, mul(2, ~_2331))
#+END_SRC

To further demonstrate the expressive power of miniKanren in this context, in
Listing [[math-constrained-expand-reduce]] we show how easy it is to perform
term reduction, expansion, or both under structural constraints on the desired
terms.  Specifically, we ask for the first ten expanded/reduced term pairs where
the expanded term is a logarithm with at least one argument.

In Listing [[math-constrained-expand-reduce]], we use src_python[:eval never]{cons}
three times to constrain the logic
variable src_python[:eval never]{expanded_term} to
only src_python[:eval never]{log} terms with an src_python[:eval never]{add}
term as the first argument, and we'll further restrict
the src_python[:eval never]{add} term to one not containing
another src_python[:eval never]{add} as its first argument.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Constrained term ``expansion''" and ``reduction''.
#+NAME: math-constrained-expand-reduce
#+BEGIN_SRC python :eval never
>>> from kanren.constraints import neq
>>>
>>>
>>> log_arg = var()
>>> first_arg = var()
>>> first_arg_car, first_arg_cdr = var(), var()
>>>
>>> res = run(10, [expanded_term, reduced_term],
>>>           eq(etuple(log, log_arg), expanded_term),
>>>           eq(etuple(add, first_arg, var()), log_arg),
>>>           conso(first_arg_car, first_arg_cdr, first_arg),
>>>           neq(first_arg_car, add),
>>>           term_walko(math_reduceo, expanded_term, reduced_term))

>>> rjust = max(map(lambda x: len(str(x[0])), res))
>>> print('\n'.join((f'{str(e):>{rjust}} == {str(r)}' for e, r in res)))
>>>                             log(add((~_771 . ~_772), (~_771 . ~_772))) == log(mul(2, (~_771 . ~_772)))
>>>               log(add(log(exp(add(~_815, ~_815))), add(~_829, ~_829))) == log(add(mul(2, ~_815), mul(2, ~_829)))
>>>                           log(add((~_771 . ~_772), add(~_851, ~_851))) == log(add((~_771 . ~_772), mul(2, ~_851)))
>>>                  log(add(~_806(add(~_869, ~_869)), add(~_887, ~_887))) == log(add(~_806(mul(2, ~_869)), mul(2, ~_887)))
>>>                    log(add((~_771 . ~_772), ~_844(add(~_909, ~_909)))) == log(add((~_771 . ~_772), ~_844(mul(2, ~_909))))
>>>                           log(add(log(exp(~_788)), add(~_935, ~_935))) == log(add(~_788, mul(2, ~_935)))
>>>                 log(add((~_771 . ~_772), log(exp(add(~_945, ~_945))))) == log(add((~_771 . ~_772), mul(2, ~_945)))
>>>           log(add(~_806(~_808, add(~_967, ~_967)), add(~_985, ~_985))) == log(add(~_806(~_808, mul(2, ~_967)), mul(2, ~_985)))
>>>           log(add((~_771 . ~_772), ~_844(~_846, add(~_1007, ~_1007)))) == log(add((~_771 . ~_772), ~_844(~_846, mul(2, ~_1007))))
>>> log(add(log(exp(log(exp(add(~_1021, ~_1021))))), add(~_1035, ~_1035))) == log(add(mul(2, ~_1021), mul(2, ~_1035)))
#+END_SRC

By properly supporting src_scheme[:eval never]{cons} semantics, we were able to
constructively express a non-trivial term constraint with only four simple
goals.

** The src_python[:eval never]{symbolic-pymc} Package
:PROPERTIES:
:CUSTOM_ID: sec:symbolic-pymc
:END:

In order to bring together miniKanren and the popular tensor libraries Theano
and TensorFlow, our project src_python[:eval never]{symbolic-pymc} provides
``meta'' type analogs for the essential tensor graph types of each ``backend'' tensor
library.  These meta types allow for more graph mutability than the ``base''
libraries themselves tend to provide.  They also allow one to use logic
variables where the base libraries wouldn't.

Basic use of src_python[:eval never]{symbolic-pymc} involves either conversion
of an existing base--i.e. Theano or TensorFlow--graph into a corresponding meta graph, or
direct construction of meta graphs that are later converted (or ``reified'') to
one of the base graph types.  miniKanren goals generally work at the meta graph level,
where--for instance--one would apply goals that unify a converted base graph
with a pure meta graph containing logic variables.

While it is possible to achieve the same results with only src_python[:eval never]{etuple}s,
meta graphs are a convenient form that allow developers to think and operate at the
standard Python object level.  They also provide a more direct means of graph
validation and setup, since checks can--and are--performed during meta graph construction,
whereas standard src_python[:eval never]{etuple}s would not be able to perform
such operations until the resulting src_python[:eval never]{etuple} is fully
constructed and evaluated.  Likewise, meta graphs are more appropriate for
specifying and obtaining derived information, like shapes and data types, for a
(sub)graph instead of miniKanren.

To demonstrate the use of src_python[:eval never]{symbolic-pymc}, we consider a
simple conjugate model constructed using PyMC3 in Listing [[beta-binom-setup]].

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: A beta-binomial conjugate model in PyMC3.
#+NAME: beta-binom-setup
#+BEGIN_SRC python :eval never
import pymc3 as pm

with pm.Model() as model:
    p = pm.Beta("p", alpha=2, beta=2)
    y = pm.Binomial("y", n=totals, p=p, observed=obs_counts)
#+END_SRC

A user will generally have some data specifying the values
for src_python[:eval never]{totals} and src_python[:eval never]{obs_counts} and
will want to estimate the posterior distribution of src_python[:eval never]{p}.
In the Bayesian world, posterior distributions are generally the object of
interest and estimation.

More specifically, we want to estimate the distribution for a rate of
success, src_python[:eval never]{p}, given a total number of
events, src_python[:eval never]{totals}, and observed
successes, src_python[:eval never]{obs_counts}, under the assumption that the
events are binomially distributed with rate src_python[:eval never]{p}.  We
let src_python[:eval never]{p} take a beta distribution prior, and that
completes our Bayesian specification of a model.

Mathematically, this simple model is stated as follows:
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:beta-binom-model}
  \begin{aligned}
    Y &\sim \operatorname{Binom}\mleft( N, p \mright)
    \\
    p &\sim \operatorname{Beta}\mleft( 2, 2 \mright)
  \end{aligned}
\end{equation}
#+END_SRC
and it has a well known closed-form posterior distribution given by
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:beta-binom-post}
  \left( p \mid Y=y \right) \sim \operatorname{Beta}\mleft( 2 + y, 2 + N - y \mright)
\end{equation}
#+END_SRC

Instead of wasting resources estimating the posterior numerically
(e.g. using src_python[:eval never]{pm.sample(model)} to run a Markov Chain
Monte Carlo sampler), we can simply extract the underlying Theano graph
from src_python[:eval never]{model} and apply a relation that represents the
underlying conjugacy and use the resulting posterior.

The general rule implied by this situation is
#+BEGIN_SRC latex
\begin{equation}
  \label{eq:beta-binom-rule}
  \begin{aligned}
    \left( \text{beta-binomial-conjugate} \right) &\quad
    \frac{
      Y \sim \operatorname{Binom}\mleft( N, p \mright), \quad
      p \sim \operatorname{Beta}\mleft( \alpha, \beta \mright)
    }{
      \left( p \mid Y=y \right) \sim \operatorname{Beta}\mleft( \alpha + y, \beta + N - y \mright)
    }
  \end{aligned}
\end{equation}
#+END_SRC

Listing [[beta-binom-convert]] converts the PyMC3 model object, src_python[:eval never]{model},
into a standard Theano graph that represents the relationship between random variables
in the model.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Converting a PyMC3 model into a Theano graph of the model's sample-space.
#+NAME: beta-binom-convert
#+BEGIN_SRC python :eval never
from symbolic_pymc.theano.pymc3 import model_graph
from symbolic_pymc.theano.utils import canonicalize


# Convert the PyMC3 graph into a symbolic-pymc graph
fgraph = model_graph(model)

# Perform a set of standard algebraic simplifications using Theano
fgraph = canonicalize(fgraph, in_place=False)
#+END_SRC

Listing [[beta-binom-goal]] uses miniKanren to construct a
goal, src_python[:eval never]{betabin_conjugateo}, that matches terms taking the
form of Equation [[eqref:eq:beta-binom-model]] in the first argument and the
resulting posterior of Equation [[eqref:eq:beta-binom-post]] in the second argument.
It makes use of both meta objects and src_python[:eval never]{etuple}s.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: A miniKanren goal that implements the beta-binomial conjugate rule in [[eqref:eq:beta-binom-rule]].
#+NAME: beta-binom-goal
#+BEGIN_SRC python :eval never
def betabin_conjugateo(x, y):
    """Replace an observed Beta-Binomial model with an unobserved posterior Beta-Binomial model."""
    obs_lv = var()

    beta_size, beta_rng, beta_name_lv = var(), var(), var()
    alpha_lv, beta_lv = var(), var()
    # We use meta objects directly to construct the "pattern" we want to match
    beta_rv_lv = mt.BetaRV(alpha_lv, beta_lv, size=beta_size, rng=beta_rng, name=beta_name_lv)

    binom_size, binom_rng, binom_name_lv = var(), var(), var()
    N_lv = var()
    binom_lv = mt.BinomialRV(N_lv, beta_rv_lv, size=binom_size, rng=binom_rng, name=binom_name_lv)

    # Here we use etuples for the output terms
    obs_sum = etuple(mt.sum, obs_lv)
    alpha_new = etuple(mt.add, alpha_lv, obs_sum)
    beta_new = etuple(mt.add, beta_lv, etuple(mt.sub, etuple(mt.sum, N_lv), obs_sum))

    beta_post_rv_lv = etuple(
        mt.BetaRV, alpha_new, beta_new, beta_size, beta_rng, name=etuple(add, beta_name_lv, "_post")
    )
    binom_new_lv = etuple(
        mt.BinomialRV,
        N_lv,
        beta_post_rv_lv,
        binom_size,
        binom_rng,
        name=etuple(add, binom_name_lv, "_post"),
    )

    return lall(eq(x, mt.observed(obs_lv, binom_lv)), eq(y, binom_new_lv))

#+END_SRC

Finally, Listing [[beta-binom-run]] shows how the goal can be applied to the model's graph
and how a new Theano graph and PyMC3 model is constructed from the output.

#+ATTR_LATEX: :float nil :placement h!
#+CAPTION: Running the beta-binomial conjugate goal and creating a PyMC3 model for the results.
#+NAME: beta-binom-run
#+BEGIN_SRC python :eval never
from symbolic_pymc.theano.pymc3 import graph_model


q = var()
res = run(1, q, betabin_conjugateo(fgraph.outputs[0], q))

expr_graph = res[0].eval_obj
fgraph_conj = expr_graph.reify()

# Convert the Theano graph into a PyMC3 model
model_conjugated = graph_model(fgraph_conj)
#+END_SRC

[[citet:WillardTourSymbolicPyMC2020]] gives a more thorough walk-through
of src_python[:eval never]{symbolic-pymc} and miniKanren--using TensorFlow
graphs.

* Discussion

Looking forward, the ``functions grimoire'' project of
[[citet:Johanssonfungrim2020]] is a great example of community-sourced and
encoded domain knowledge.  It serves as a prime example of an independently
developed, high-level knowledge encoding effort that could provide relations
deployable by miniKanren within another computationally-focused system like
TensorFlow or JAX.  These are the cross-overs with made possible by the ``glue''
that could be miniKanren.

One currently unexplored area involves interactions between miniKanren and
symbolic algebra libraries.  We don't expect miniKanren to replace symbolic
algebra libraries, especially not when many symbolic algebra operations are not
needed in a relational capacity, and there is an immediate place for all the
standard algebraic simplifications offered by libraries like SymPy
[[citep:sympydevelopmentteam_sympy_2014]].  Distinctions arise when one
considers how ``orchestratable'' said symbolic algebra libraries actually are.
Fortunately, SymPy can be directly integrated with a Python version of
miniKanren.  This opens up the possibility of applying advanced algebraic
operations within goals.  One example could involve the use of
inverse-Laplace transforms for finding scale mixture decompositions of random
variables when they aren't directly encoded in an existing goal.

Perhaps another concrete example of how a library like SymPy could be leveraged
by miniKanren is given by the system described in
[[citet:Waliahighlevelinferencealgorithms2018]]; if this system were implemented
in miniKanren, an implementation of the type rules might be straight-forward to
express /and/ the ~integrate~ steps could be easily outsourced to SymPy.

Regarding miniKanren itself, consider the following idiom:
#+BEGIN_SRC scheme :eval never
(conde ((== lhs match-form-1)
        (== rhs replace-form-1))
       ((== lhs match-form-2)
        (== rhs replace-form-2))
       ...)
#+END_SRC

src_scheme[:eval never]{conde}s of this form are natural for encoding
identities of the form \(\text{match-form-1} = \text{replace-form-1}\) that
relate src_scheme[:eval never]{lhs}, src_scheme[:eval never]{rhs}.
They also appear in implementations of relational interpreters where they
encode the supported forms of a target language (e.g. variable assignment,
conditionals, etc.)

These src_scheme[:eval never]{conde} idioms comprise a large portion of the
miniKanren work implied here, and their size could grow very quickly over time.
This leads to performance questions that are possibly answered by work on guided
search [[citep:SwordsGuidedSearchminiKanren, ZhangNeuralGuidedConstraint2018]]
and discerning src_scheme[:eval never]{conde} branch selection
[[citep:BoskinSurprisinglyCompetitiveConditional2018]].

There is also little reason to think that a
single src_scheme[:eval never]{conde} will--or even /should/--encode most of a
system's implemented identities, so a means of compiling goals--say--for the
purposes of merging branches might be worth considering, as well.

With equational identities nicely encoded by src_scheme[:eval never]{conde}
forms, there's also the possibility that the rewrite-completion algorithms
mentioned in Section [[#sec:want-to-do]] could be applied automatically.  When a
complete and reduced rewrite system can be generated from
a src_scheme[:eval never]{conde}, it would be interesting to know whether or not
the resulting system improves the general performance of miniKanren.

Also, is it possible that some cases of non-terminating goal orderings could be
avoided by completion? Likewise, could the results of completion be used to
produce a new, equivalent src_scheme[:eval never]{conde} that results in fewer
goal evaluations and/or failed branches?

Alternatively, is it possible that miniKanren could be utilized by a
completion algorithm itself so that it produces potentially relevant results
when it otherwise wouldn't terminate (e.g. via infinite goal streams)?
What are the advantages of performing completion in a relational fashion, and
what unique elements can miniKanren provide to that situation (e.g. easier
implementation of experimental completion algorithms)?

Proving rewrite termination and completion itself can involve SAT problems
[[citep:EndrullisMatrixinterpretationsproving2008, KleinMaximalcompletion2011]];
can miniKanren's constraint capabilities--among other things--be applied in this
area?

Our Python implementation of miniKanren comes with experimental
support for associative and commutative (AC) relations.
We've found utility in assigning these two properties to existing operators from
other libraries (e.g. addition operators in Theano and TensorFlow) as a means
of adding flexibility to the exact representation of graphs.  This is especially
important in instances where graph normalization isn't entirely consistent or
available via the targeted graph backend (e.g. TensorFlow).

The process of implementing these AC relations has opened a few questions that
cannot be properly treated here.  Questions such as "How can operators with
arbitrary--but known and fixed--arities be efficiently supported?" and "How can
we overcome some of the goal ordering issues that arise due to commutativity?".

In [[citep:WillardPullRequest272020]], we address the latter question with a
form of ``groundedness''-based term reordering goal.  This reordering is performed on
the src_scheme[:eval never]{cdr} sub-terms as a relation is applied between term
graphs, since the order in which a relation is applied to corresponding
sub-terms is--generally--immaterial.  In other words, when walking a
goal src_scheme[:eval never]{relo} between the
lists src_scheme[:eval never]{'(a b)} and src_scheme[:eval never]{'(c 2)}, for fresh
variables src_scheme[:eval never]{a}, src_scheme[:eval never]{b}, and src_scheme[:eval never]{c}, the
goal can be applied in any order, e.g. src_scheme[:eval never]{(relo a c)}
then src_scheme[:eval never]{(relo b 2)}, or src_scheme[:eval never]{(relo b 2)}
then src_scheme[:eval never]{(relo a c)}.  When--for
instance--src_scheme[:eval never]{(relo a b)} diverges because both arguments
are fresh, while src_scheme[:eval never]{(relo b 2)} fails, a walk that performs
the former ordering will diverge, while one that does the latter will fail.  The
``groundedness'' ordering goal simply reorders the corresponding pairs according
to how grounded they are to arrive at the non-diverging order of application.

Finally, we would like to point out the potential for an exciting ``feedback loop'': as
statistical modeling improves the processing of miniKanren
[[citep:ZhangNeuralGuidedConstraint2018]], miniKanren can also improve the
process of statistical modeling.

:acks:
The author would like to thank Jason Hemann and William Byrd for their
invaluable input and inspiring work.
:end:

#+BIBLIOGRAPHYSTYLE: plainnat
#+BIBLIOGRAPHY: ../tex/ICFP2020.bib
